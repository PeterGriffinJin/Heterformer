{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de08fb5",
   "metadata": {},
   "source": [
    "## read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train file\n",
    "train_set = []\n",
    "\n",
    "with open('raw/train.tsv') as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        train_set.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3767044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read val file\n",
    "val_set = []\n",
    "\n",
    "with open('raw/val.tsv') as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        val_set.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test file\n",
    "test_set = []\n",
    "\n",
    "with open('raw/test.tsv') as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        test_set.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90902e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef318f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate author_id2idx_dict, tag_id2idx_dict, mention_id2idx_dict based on train set\n",
    "\n",
    "author_id2idx_dict = {}\n",
    "tag_id2idx_dict = {}\n",
    "mention_id2idx_dict = {}\n",
    "poi_neighbour_dict = defaultdict(list)\n",
    "tweet_id2text_dict = {}\n",
    "\n",
    "mention_neighbour = defaultdict(list)\n",
    "tag_neighbour = defaultdict(list)\n",
    "author_neighbour = defaultdict(list)\n",
    "\n",
    "for t in tqdm(train_set):\n",
    "    poi_neighbour_dict[t['poi_id']].append(t['tweet_id'])\n",
    "    tweet_id2text_dict[t['tweet_id']] = t['tweet_text']\n",
    "    \n",
    "    author_neighbour[t['author_id']].append(t['tweet_text'])\n",
    "    if t['author_id'] not in author_id2idx_dict:\n",
    "        author_id2idx_dict[t['author_id']] = len(author_id2idx_dict)\n",
    "    \n",
    "    for tag in t['tags']:\n",
    "        tag_neighbour[tag].append(t['tweet_text'])\n",
    "        if tag not in tag_id2idx_dict:\n",
    "            tag_id2idx_dict[tag] = len(tag_id2idx_dict)\n",
    "            \n",
    "    for mention in t['mentions']:\n",
    "        mention_neighbour[mention].append(t['tweet_text'])\n",
    "        if mention not in mention_id2idx_dict:\n",
    "            mention_id2idx_dict[mention] = len(mention_id2idx_dict)\n",
    "    \n",
    "print(f'Author:{len(author_id2idx_dict)}, Tags:{len(tag_id2idx_dict)}, Mentions:{len(mention_id2idx_dict)}, POI:{len(poi_neighbour_dict)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629c42fc",
   "metadata": {},
   "source": [
    "## Generate Train/Val/Test with no filtering on metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083d1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate official train file\n",
    "# tweet_text \\t ... blank_tweet_neighbour ... \\t mention_id1 \\t mention_id2 \\t tag_id1 \\t tag_id2 \\t tag_id3 \\t author_id $$ poi_text \\t tweet_id_1 \\t ... \\t tweet_id_6 \\t ... blank_notext_neighbour ... \\n\n",
    "# remember that we add blank position to make tweet_text node and poi node exactly the same\n",
    "# this is used to generate train_text.tsv\n",
    "\n",
    "mention_neighbours = 2\n",
    "tag_neighbours = 3\n",
    "tweet_neighbours = 6\n",
    "\n",
    "blank_for_tweet = [''] * tweet_neighbours\n",
    "blank_for_poi = ['-1'] * (mention_neighbours + tag_neighbours + 1)\n",
    "\n",
    "with open('Tweet_text/train_text.tsv','w') as fout:\n",
    "    for t in tqdm(train_set):\n",
    "        # generate tweet center node\n",
    "        ## sample mentions\n",
    "        m = [mention_id2idx_dict[mm] for mm in t['mentions']]\n",
    "        random.shuffle(m)\n",
    "        \n",
    "        if len(m) >= mention_neighbours:\n",
    "            sampled_mention_n = m[:mention_neighbours]\n",
    "        else:\n",
    "            sampled_mention_n = m + [-1] * (mention_neighbours - len(m))\n",
    "        sampled_mention_n = [str(mm) for mm in sampled_mention_n]\n",
    "        \n",
    "        ## sample tags\n",
    "        tags = [tag_id2idx_dict[tt] for tt in t['tags']]\n",
    "        random.shuffle(tags)\n",
    "        \n",
    "        if len(tags) >= tag_neighbours:\n",
    "            sampled_tag_n = tags[:tag_neighbours]\n",
    "        else:\n",
    "            sampled_tag_n = tags + [-1] * (tag_neighbours - len(tags))\n",
    "        sampled_tag_n = [str(tt) for tt in sampled_tag_n]\n",
    "        \n",
    "        ## concate for tweet\n",
    "        tw = [t['tweet_text']] + blank_for_tweet + sampled_mention_n + sampled_tag_n + [str(author_id2idx_dict[t['author_id']])]\n",
    "\n",
    "        # generate poi center node\n",
    "        tw_n = list(poi_neighbour_dict[t['poi_id']])\n",
    "        tw_n.pop(tw_n.index(t['tweet_id']))\n",
    "        tw_n = [tweet_id2text_dict[tid] for tid in tw_n]\n",
    "        random.shuffle(tw_n)\n",
    "        if len(tw_n) >= tweet_neighbours:\n",
    "            sampled_twitter_n = tw_n[:tweet_neighbours]\n",
    "        else:\n",
    "            sampled_twitter_n = tw_n + [''] * (tweet_neighbours - len(tw_n))\n",
    "                \n",
    "        ## concate for poi\n",
    "        poi = [t['poi_text']] + sampled_twitter_n + blank_for_poi\n",
    "        \n",
    "        fout.write('\\t'.join(tw) + '\\$\\$'+'\\t'.join(poi)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1207b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate official validation set\n",
    "# tweet_text \\t ... blank_tweet_neighbour ... \\t mention_id1 \\t mention_id2 \\t tag_id1 \\t tag_id2 \\t tag_id3 \\t author_id $$ poi_text \\t tweet_id_1 \\t ... \\t tweet_id_6 \\t ... blank_notext_neighbour ... \\n\n",
    "# remember that we add blank position to make tweet_text node and poi node exactly the same\n",
    "# this is used to generate val_text.tsv\n",
    "\n",
    "mention_neighbours = 2\n",
    "tag_neighbours = 3\n",
    "tweet_neighbours = 6\n",
    "\n",
    "blank_for_tweet = [''] * tweet_neighbours\n",
    "blank_for_poi = ['-1'] * (mention_neighbours + tag_neighbours + 1)\n",
    "\n",
    "with open('Tweet_text/val_text.tsv','w') as fout:\n",
    "    for t in tqdm(val_set):\n",
    "        # generate tweet center node\n",
    "        ## sample mentions\n",
    "        m = [mention_id2idx_dict[mm] for mm in t['mentions'] if mm in mention_id2idx_dict]\n",
    "        random.shuffle(m)\n",
    "        \n",
    "        if len(m) >= mention_neighbours:\n",
    "            sampled_mention_n = m[:mention_neighbours]\n",
    "        else:\n",
    "            sampled_mention_n = m + [-1] * (mention_neighbours - len(m))\n",
    "        sampled_mention_n = [str(mm) for mm in sampled_mention_n]\n",
    "        \n",
    "        ## sample tags\n",
    "        tags = [tag_id2idx_dict[tt] for tt in t['tags'] if tt in tag_id2idx_dict]\n",
    "        random.shuffle(tags)\n",
    "        \n",
    "        if len(tags) >= tag_neighbours:\n",
    "            sampled_tag_n = tags[:tag_neighbours]\n",
    "        else:\n",
    "            sampled_tag_n = tags + [-1] * (tag_neighbours - len(tags))\n",
    "        sampled_tag_n = [str(tt) for tt in sampled_tag_n]\n",
    "        \n",
    "        ## concate for tweet\n",
    "        if t['author_id'] in author_id2idx_dict:\n",
    "            author = [str(author_id2idx_dict[t['author_id']])]\n",
    "        else:\n",
    "            author = ['-1']\n",
    "        tw = [t['tweet_text']] + blank_for_tweet + sampled_mention_n + sampled_tag_n + author\n",
    "\n",
    "        # generate poi center node\n",
    "        if t['poi_id'] in poi_neighbour_dict:\n",
    "            tw_n = list(poi_neighbour_dict[t['poi_id']])\n",
    "            tw_n = [tweet_id2text_dict[tid] for tid in tw_n]\n",
    "            random.shuffle(tw_n)\n",
    "        else:\n",
    "            tw_n = []\n",
    "        if len(tw_n) >= tweet_neighbours:\n",
    "            sampled_twitter_n = tw_n[:tweet_neighbours]\n",
    "        else:\n",
    "            sampled_twitter_n = tw_n + [''] * (tweet_neighbours - len(tw_n))\n",
    "        \n",
    "        ## concate for poi\n",
    "        poi = [t['poi_text']] + sampled_twitter_n + blank_for_poi\n",
    "        \n",
    "        fout.write('\\t'.join(tw) + '\\$\\$'+'\\t'.join(poi)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate official test set\n",
    "# tweet_text \\t ... blank_tweet_neighbour ... \\t mention_id1 \\t mention_id2 \\t tag_id1 \\t tag_id2 \\t tag_id3 \\t author_id $$ poi_text \\t tweet_id_1 \\t ... \\t tweet_id_6 \\t ... blank_notext_neighbour ... \\n\n",
    "# remember that we add blank position to make tweet_text node and poi node exactly the same\n",
    "# this is used to generate test_text.tsv\n",
    "\n",
    "mention_neighbours = 2\n",
    "tag_neighbours = 3\n",
    "tweet_neighbours = 6\n",
    "\n",
    "blank_for_tweet = [''] * tweet_neighbours\n",
    "blank_for_poi = ['-1'] * (mention_neighbours + tag_neighbours + 1)\n",
    "\n",
    "with open('Tweet_text/test_text.tsv','w') as fout:\n",
    "    for t in tqdm(test_set):\n",
    "        # generate tweet center node\n",
    "        ## sample mentions\n",
    "        m = [mention_id2idx_dict[mm] for mm in t['mentions'] if mm in mention_id2idx_dict]\n",
    "        random.shuffle(m)\n",
    "        \n",
    "        if len(m) >= mention_neighbours:\n",
    "            sampled_mention_n = m[:mention_neighbours]\n",
    "        else:\n",
    "            sampled_mention_n = m + [-1] * (mention_neighbours - len(m))\n",
    "        sampled_mention_n = [str(mm) for mm in sampled_mention_n]\n",
    "        \n",
    "        ## sample tags\n",
    "        tags = [tag_id2idx_dict[tt] for tt in t['tags'] if tt in tag_id2idx_dict]\n",
    "        random.shuffle(tags)\n",
    "        \n",
    "        if len(tags) >= tag_neighbours:\n",
    "            sampled_tag_n = tags[:tag_neighbours]\n",
    "        else:\n",
    "            sampled_tag_n = tags + [-1] * (tag_neighbours - len(tags))\n",
    "        sampled_tag_n = [str(tt) for tt in sampled_tag_n]\n",
    "        \n",
    "        ## concate for tweet\n",
    "        if t['author_id'] in author_id2idx_dict:\n",
    "            author = [str(author_id2idx_dict[t['author_id']])]\n",
    "        else:\n",
    "            author = ['-1']\n",
    "        tw = [t['tweet_text']] + blank_for_tweet + sampled_mention_n + sampled_tag_n + author\n",
    "\n",
    "        # generate poi center node\n",
    "        if t['poi_id'] in poi_neighbour_dict:\n",
    "            tw_n = list(poi_neighbour_dict[t['poi_id']])\n",
    "            tw_n = [tweet_id2text_dict[tid] for tid in tw_n]\n",
    "            random.shuffle(tw_n)\n",
    "        else:\n",
    "            tw_n = []\n",
    "        if len(tw_n) >= tweet_neighbours:\n",
    "            sampled_twitter_n = tw_n[:tweet_neighbours]\n",
    "        else:\n",
    "            sampled_twitter_n = tw_n + [''] * (tweet_neighbours - len(tw_n))\n",
    "        \n",
    "        ## concate for poi\n",
    "        poi = [t['poi_text']] + sampled_twitter_n + blank_for_poi\n",
    "        \n",
    "        fout.write('\\t'.join(tw) + '\\$\\$'+'\\t'.join(poi)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ea681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save anthor_num, tags_num, mentions_num\n",
    "pickle.dump([len(author_id2idx_dict), len(tag_id2idx_dict), len(mention_id2idx_dict)], open('Tweet_text/mta_num.pkl', 'wb'))\n",
    "pickle.dump(author_id2idx_dict, open('Tweet_text/author_id2idx_dict.pkl', 'wb'))\n",
    "pickle.dump(tag_id2idx_dict, open('Tweet_text/tag_id2idx_dict.pkl', 'wb'))\n",
    "pickle.dump(mention_id2idx_dict, open('Tweet_text/mention_id2idx_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30fdf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mention_neighbour, open('Tweet_text/mention_neighbour.pkl', 'wb'))\n",
    "pickle.dump(tag_neighbour, open('Tweet_text/tag_neighbour.pkl', 'wb'))\n",
    "pickle.dump(author_neighbour, open('Tweet_text/author_neighbour.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c62187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "848b3b87",
   "metadata": {},
   "source": [
    "## Generate filtered train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate official train file\n",
    "# tweet_text \\t ... blank_tweet_neighbour ... \\t mention_id1 \\t mention_id2 \\t tag_id1 \\t tag_id2 \\t tag_id3 \\t author_id $$ poi_text \\t tweet_id_1 \\t ... \\t tweet_id_6 \\t ... blank_notext_neighbour ... \\n\n",
    "# remember that we add blank position to make tweet_text node and poi node exactly the same\n",
    "# this is used to generate train_textf.tsv\n",
    "\n",
    "mention_neighbours = 2\n",
    "tag_neighbours = 3\n",
    "tweet_neighbours = 6\n",
    "\n",
    "blank_for_tweet = [''] * tweet_neighbours\n",
    "blank_for_poi = ['-1'] * (mention_neighbours + tag_neighbours + 1)\n",
    "\n",
    "with open(f'Tweet_text/train_textf{filter_num}.tsv','w') as fout:\n",
    "    for t in tqdm(train_set):\n",
    "        # generate tweet center node\n",
    "        ## sample mentions\n",
    "        m = [mention_id2idx_dict[mm] for mm in t['mentions'] if len(mention_neighbour[mm]) >= filter_num]\n",
    "        random.shuffle(m)\n",
    "        \n",
    "        if len(m) >= mention_neighbours:\n",
    "            sampled_mention_n = m[:mention_neighbours]\n",
    "        else:\n",
    "            sampled_mention_n = m + [-1] * (mention_neighbours - len(m))\n",
    "        sampled_mention_n = [str(mm) for mm in sampled_mention_n]\n",
    "        \n",
    "        ## sample tags\n",
    "        tags = [tag_id2idx_dict[tt] for tt in t['tags'] if len(tag_neighbour[tt]) >= filter_num]\n",
    "        random.shuffle(tags)\n",
    "        \n",
    "        if len(tags) >= tag_neighbours:\n",
    "            sampled_tag_n = tags[:tag_neighbours]\n",
    "        else:\n",
    "            sampled_tag_n = tags + [-1] * (tag_neighbours - len(tags))\n",
    "        sampled_tag_n = [str(tt) for tt in sampled_tag_n]\n",
    "        \n",
    "        ## sample author\n",
    "        if len(tag_neighbour[t['author_id']]) >= filter_num:\n",
    "            author = [str(author_id2idx_dict[t['author_id']])]\n",
    "        else:\n",
    "            author = [str(-1)]\n",
    "        \n",
    "        ## concate for tweet\n",
    "        tw = [t['tweet_text']] + blank_for_tweet + sampled_mention_n + sampled_tag_n + author\n",
    "\n",
    "        # generate poi center node\n",
    "        tw_n = list(poi_neighbour_dict[t['poi_id']])\n",
    "        tw_n.pop(tw_n.index(t['tweet_id']))\n",
    "        tw_n = [tweet_id2text_dict[tid] for tid in tw_n]\n",
    "        random.shuffle(tw_n)\n",
    "        if len(tw_n) >= tweet_neighbours:\n",
    "            sampled_twitter_n = tw_n[:tweet_neighbours]\n",
    "        else:\n",
    "            sampled_twitter_n = tw_n + [''] * (tweet_neighbours - len(tw_n))\n",
    "                \n",
    "        ## concate for poi\n",
    "        poi = [t['poi_text']] + sampled_twitter_n + blank_for_poi\n",
    "        \n",
    "        fout.write('\\t'.join(tw) + '\\$\\$'+'\\t'.join(poi)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495e7b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate official validation set\n",
    "# tweet_text \\t ... blank_tweet_neighbour ... \\t mention_id1 \\t mention_id2 \\t tag_id1 \\t tag_id2 \\t tag_id3 \\t author_id $$ poi_text \\t tweet_id_1 \\t ... \\t tweet_id_6 \\t ... blank_notext_neighbour ... \\n\n",
    "# remember that we add blank position to make tweet_text node and poi node exactly the same\n",
    "# this is used to generate val_textf.tsv\n",
    "\n",
    "mention_neighbours = 2\n",
    "tag_neighbours = 3\n",
    "tweet_neighbours = 6\n",
    "\n",
    "blank_for_tweet = [''] * tweet_neighbours\n",
    "blank_for_poi = ['-1'] * (mention_neighbours + tag_neighbours + 1)\n",
    "\n",
    "with open(f'Tweet_text/val_textf{filter_num}.tsv','w') as fout:\n",
    "    for t in tqdm(val_set):\n",
    "        # generate tweet center node\n",
    "        ## sample mentions\n",
    "        m = [mention_id2idx_dict[mm] for mm in t['mentions'] if mm in mention_id2idx_dict]\n",
    "        random.shuffle(m)\n",
    "        \n",
    "        if len(m) >= mention_neighbours:\n",
    "            sampled_mention_n = m[:mention_neighbours]\n",
    "        else:\n",
    "            sampled_mention_n = m + [-1] * (mention_neighbours - len(m))\n",
    "        sampled_mention_n = [str(mm) for mm in sampled_mention_n]\n",
    "        \n",
    "        ## sample tags\n",
    "        tags = [tag_id2idx_dict[tt] for tt in t['tags'] if tt in tag_id2idx_dict]\n",
    "        random.shuffle(tags)\n",
    "        \n",
    "        if len(tags) >= tag_neighbours:\n",
    "            sampled_tag_n = tags[:tag_neighbours]\n",
    "        else:\n",
    "            sampled_tag_n = tags + [-1] * (tag_neighbours - len(tags))\n",
    "        sampled_tag_n = [str(tt) for tt in sampled_tag_n]\n",
    "        \n",
    "        ## concate for tweet\n",
    "        if t['author_id'] in author_id2idx_dict:\n",
    "            author = [str(author_id2idx_dict[t['author_id']])]\n",
    "        else:\n",
    "            author = ['-1']\n",
    "        tw = [t['tweet_text']] + blank_for_tweet + sampled_mention_n + sampled_tag_n + author\n",
    "\n",
    "        # generate poi center node\n",
    "        if t['poi_id'] in poi_neighbour_dict:\n",
    "            tw_n = list(poi_neighbour_dict[t['poi_id']])\n",
    "            tw_n = [tweet_id2text_dict[tid] for tid in tw_n]\n",
    "            random.shuffle(tw_n)\n",
    "        else:\n",
    "            tw_n = []\n",
    "        if len(tw_n) >= tweet_neighbours:\n",
    "            sampled_twitter_n = tw_n[:tweet_neighbours]\n",
    "        else:\n",
    "            sampled_twitter_n = tw_n + [''] * (tweet_neighbours - len(tw_n))\n",
    "        \n",
    "        ## concate for poi\n",
    "        poi = [t['poi_text']] + sampled_twitter_n + blank_for_poi\n",
    "        \n",
    "        fout.write('\\t'.join(tw) + '\\$\\$'+'\\t'.join(poi)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate official test set\n",
    "# tweet_text \\t ... blank_tweet_neighbour ... \\t mention_id1 \\t mention_id2 \\t tag_id1 \\t tag_id2 \\t tag_id3 \\t author_id $$ poi_text \\t tweet_id_1 \\t ... \\t tweet_id_6 \\t ... blank_notext_neighbour ... \\n\n",
    "# remember that we add blank position to make tweet_text node and poi node exactly the same\n",
    "# this is used to generate test_text.tsv\n",
    "\n",
    "mention_neighbours = 2\n",
    "tag_neighbours = 3\n",
    "tweet_neighbours = 6\n",
    "\n",
    "blank_for_tweet = [''] * tweet_neighbours\n",
    "blank_for_poi = ['-1'] * (mention_neighbours + tag_neighbours + 1)\n",
    "\n",
    "with open(f'Tweet_text/test_textf{filter_num}.tsv','w') as fout:\n",
    "    for t in tqdm(test_set):\n",
    "        # generate tweet center node\n",
    "        ## sample mentions\n",
    "        m = [mention_id2idx_dict[mm] for mm in t['mentions'] if mm in mention_id2idx_dict]\n",
    "        random.shuffle(m)\n",
    "        \n",
    "        if len(m) >= mention_neighbours:\n",
    "            sampled_mention_n = m[:mention_neighbours]\n",
    "        else:\n",
    "            sampled_mention_n = m + [-1] * (mention_neighbours - len(m))\n",
    "        sampled_mention_n = [str(mm) for mm in sampled_mention_n]\n",
    "        \n",
    "        ## sample tags\n",
    "        tags = [tag_id2idx_dict[tt] for tt in t['tags'] if tt in tag_id2idx_dict]\n",
    "        random.shuffle(tags)\n",
    "        \n",
    "        if len(tags) >= tag_neighbours:\n",
    "            sampled_tag_n = tags[:tag_neighbours]\n",
    "        else:\n",
    "            sampled_tag_n = tags + [-1] * (tag_neighbours - len(tags))\n",
    "        sampled_tag_n = [str(tt) for tt in sampled_tag_n]\n",
    "        \n",
    "        ## concate for tweet\n",
    "        if t['author_id'] in author_id2idx_dict:\n",
    "            author = [str(author_id2idx_dict[t['author_id']])]\n",
    "        else:\n",
    "            author = ['-1']\n",
    "        tw = [t['tweet_text']] + blank_for_tweet + sampled_mention_n + sampled_tag_n + author\n",
    "\n",
    "        # generate poi center node\n",
    "        if t['poi_id'] in poi_neighbour_dict:\n",
    "            tw_n = list(poi_neighbour_dict[t['poi_id']])\n",
    "            tw_n = [tweet_id2text_dict[tid] for tid in tw_n]\n",
    "            random.shuffle(tw_n)\n",
    "        else:\n",
    "            tw_n = []\n",
    "        if len(tw_n) >= tweet_neighbours:\n",
    "            sampled_twitter_n = tw_n[:tweet_neighbours]\n",
    "        else:\n",
    "            sampled_twitter_n = tw_n + [''] * (tweet_neighbours - len(tw_n))\n",
    "        \n",
    "        ## concate for poi\n",
    "        poi = [t['poi_text']] + sampled_twitter_n + blank_for_poi\n",
    "        \n",
    "        fout.write('\\t'.join(tw) + '\\$\\$'+'\\t'.join(poi)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d4c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435700cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
